{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and notes\n",
    "\n",
    "This prepares the input data for the **XGB Model**, where for each date, run a model, our stores are our targets *y*. Many of our predictors *X* are based on that store's history of customers. We can also add general information based on stores in a similar location and genre, plus general information from all stores for the date of the year etc.\n",
    "\n",
    "We then make a new model for each new day that we want to predict, e.g. we can predict tomorrow first, then 明後日,　明々後日 etc. by a new model each time. This can be done by looping over the code.\n",
    "\n",
    "A Chainer LSTM and time-series ensemble model will be added later. However, regression seems to work better than time series due to lots of gaps in the data.\n",
    "\n",
    "See `vizualization.py` in this directory for some justifications of this stuff.\n",
    "\n",
    "## Okay, let's get started\n",
    "\n",
    "Standard startup fluff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# a list of stores that are in the train but not the test data\n",
    "# can drop them for better performance\n",
    "missing_stores = [\"air_d63cfa6d6ab78446\",\"air_b2d8bc9c88b85f96\",\"air_0ead98dd07e7a82a\",\"air_cb083b4789a8d3a2\",\"air_cf22e368c1a71d53\",\"air_d0a7bd3339c3d12a\",\"air_229d7e508d9f1b5e\",\"air_2703dcb33192b181\"]\n",
    "\n",
    "# a list of reservations that seem to be data entry errors\n",
    "# not actually using these, so don't need to drop them, but may be useful in future\n",
    "#bad_res = ['air_2a485b92210c98b5', 'air_465bddfed3353b23',       'air_56cebcbd6906e04c', 'air_900d755ebd2f7bbd',       'air_900d755ebd2f7bbd', 'air_900d755ebd2f7bbd',       'air_a17f0778617c76e2', 'air_a17f0778617c76e2',       'air_a17f0778617c76e2', 'air_a17f0778617c76e2',       'air_a17f0778617c76e2', 'air_b439391e72899756',       'air_e7fbee4e3cfe65c5', 'air_e7fbee4e3cfe65c5',       'air_e7fbee4e3cfe65c5', 'air_e7fbee4e3cfe65c5']\n",
    "#bad_res_dates = [datetime.date(2017, 1, 18), datetime.date(2017, 1, 9),       datetime.date(2017, 3, 19), datetime.date(2017, 3, 3),       datetime.date(2017, 3, 7), datetime.date(2017, 3, 24),       datetime.date(2016, 11, 10), datetime.date(2016, 11, 11),       datetime.date(2016, 12, 18), datetime.date(2017, 1, 5),       datetime.date(2017, 3, 16), datetime.date(2017, 2, 23),       datetime.date(2017, 2, 3), datetime.date(2017, 2, 4),       datetime.date(2017, 2, 5), datetime.date(2017, 2, 7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Prediction Date\n",
    "\n",
    "Set the point that we are predicting from (i.e. the first unknown day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tabular data for 2017-03-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "one_day = timedelta(days=1)\n",
    "\n",
    "# The test set will be:\n",
    "#prediction_date = datetime(2017,4,23)\n",
    "# For now we use as a train set:\n",
    "prediction_date = datetime(2017,4,23) - timedelta(days=39)\n",
    "\n",
    "# Set how many extra days ahead of this we will predict\n",
    "\n",
    "# SCRIPT VERSION\n",
    "# from sys import argv\n",
    "# if (len(argv) > 3) or (len(argv) < 2):\n",
    "#     print(\"Usage: days_ahead (run_calculation)\")\n",
    "#     raise SystemExit\n",
    "# run_calculation = (len(argv) == 3)\n",
    "# days_ahead = int(argv[1])\n",
    "\n",
    "# NOTEBOOK VERSION\n",
    "days_ahead = 1\n",
    "\n",
    "today = prediction_date + timedelta(days=days_ahead)\n",
    "\n",
    "print(\"Building tabular data for\",  today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Genre of cuisine\n",
    "1. Location\n",
    " * todofuken\n",
    " * ku/shi\n",
    " * throw away more detailed info?\n",
    "1. Reservations\n",
    " * reserved for this day in AIR\n",
    " * reserved for this day in HPG\n",
    " * **note**: cutoff registrations info after prediction_date, e.g. don't use on the day reservations\n",
    " * **note**: AIR reservations follow a weird pattern, could be related to system downtime or something. Could introduce a variable to account for this?\n",
    "1. Customers\n",
    " * each day for ~2 weeks before\n",
    " * average for last week for ~4 weeks before\n",
    " * trend for last week (average), last month, last year [taylor series!]\n",
    " * average for this store on this day of the week (past ~6 months)\n",
    "1. is a holiday or not\n",
    "1. is golden week * is in Tokyo\n",
    "1. further combinations of golden week & genre?\n",
    "1. Average info from other stores\n",
    "  1. for average over all stores in this location:\n",
    "    * customers on this day last year \n",
    "    * customers on this day of the week (normalized?)\n",
    "    * customers over the last week\n",
    "    * customers on this day last week \n",
    "  1.  for average over all stores in this genre:\n",
    "    *    same as above (normalized to the average customers for this genre?) \n",
    "  1. for average over all stores:\n",
    "    * customers on this day last year (normalized)\n",
    "\n",
    "### Set up the initial dataset\n",
    "Remember that our targets *y* are customers for each **store**, and we will re-train for different days. So, the predictor set *X* should be information relevant to each store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read store data\n",
    "stores = pd.read_csv(\"../input/air_store_info.csv\",\n",
    "                    index_col = [\"air_store_id\"])\n",
    "\n",
    "# drop missing stores\n",
    "stores.drop(missing_stores, inplace=True)\n",
    "\n",
    "# forget about latitude and longitude (area is more important?)\n",
    "stores.drop([\"latitude\", \"longitude\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = stores[\"air_area_name\"].map(str.split).values\n",
    "\n",
    "todofuken, kushi = [], []\n",
    "for store in address:\n",
    "    todofuken.append(store[0])\n",
    "    kushi.append(store[1])\n",
    "\n",
    "stores.drop([\"air_area_name\"], axis=1, inplace=True)\n",
    "stores = stores.assign(todofuken = todofuken, kushi = kushi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reservations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read reservations data (AIR = 6 MB, HPG = 120 MB)\n",
    "reservations_air = pd.read_csv(\"../input/air_reserve.csv\",\n",
    "                              parse_dates=[\"visit_datetime\", \"reserve_datetime\"])\n",
    "reservations_hpg = pd.read_csv(\"../input/hpg_reserve.csv\",\n",
    "                              parse_dates=[\"visit_datetime\", \"reserve_datetime\"])\n",
    "\n",
    "# drop any reservations from ON or AFTER the prediction date,\n",
    "# e.g. on-the-day reservations, because we won't have this info\n",
    "# in the test set.\n",
    "reservations_air = reservations_air[ reservations_air[\"reserve_datetime\"].map(datetime.date) < prediction_date.date() ]\n",
    "reservations_hpg = reservations_hpg[ reservations_hpg[\"reserve_datetime\"].map(datetime.date) < prediction_date.date() ]\n",
    "\n",
    "# otherwise not using the time of making the reservation, for now\n",
    "reservations_air.drop([\"reserve_datetime\"], axis=1, inplace=True)\n",
    "reservations_hpg.drop([\"reserve_datetime\"], axis=1, inplace=True)\n",
    "\n",
    "# We don't care about HPG reservations for most of the HPG data.\n",
    "# So we can throw most of it away, but keep the ones that are reservations\n",
    "# for restaurants in the AIR system.\n",
    "\n",
    "# Assign AIR store number to HPG reservations data\n",
    "store_id = pd.read_csv(\"../input/store_id_relation.csv\")\n",
    "store_id.set_index([\"hpg_store_id\"], inplace=True)\n",
    "\n",
    "def get_air_store(hpg_store):\n",
    "    if hpg_store in store_id.index:\n",
    "        return store_id.loc[hpg_store,\"air_store_id\"]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "air_ids = reservations_hpg[\"hpg_store_id\"].map(get_air_store).values\n",
    "reservations_hpg = reservations_hpg.assign(air_store_id = air_ids)\n",
    "\n",
    "# prune any HPG reservations that aren't for stores in AIR\n",
    "reservations_hpg.dropna(axis=0, inplace=True)\n",
    "# also drop the HPG number, we don't need it anymore\n",
    "reservations_hpg.drop([\"hpg_store_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get reservations for this day\n",
    "res_air = []\n",
    "res_hpg = []\n",
    "\n",
    "#  Pick out all reservations for each store\n",
    "for store in stores.index:\n",
    "    res = reservations_air[reservations_air[\"air_store_id\"] == store]\n",
    "\n",
    "    # pick out reservations for today    \n",
    "    res = res[ res[\"visit_datetime\"].map(datetime.date) == today.date() ]\n",
    "    \n",
    "    # sum reservations for today\n",
    "    res_air.append( res[\"reserve_visitors\"].sum() )\n",
    "    \n",
    "    # same steps for HPG\n",
    "    res = reservations_hpg[reservations_hpg[\"air_store_id\"] == store]\n",
    "    res = res[ res[\"visit_datetime\"].map(datetime.date) == today.date() ]\n",
    "    res_hpg.append( res[\"reserve_visitors\"].sum() )\n",
    "\n",
    "# Add air reservations for each store\n",
    "stores = stores.assign(res_air = res_air, res_hpg = res_hpg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer History\n",
    "\n",
    "### Read in visitor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total visits: 250468\n",
      "Number of unique stores after drop: 821\n"
     ]
    }
   ],
   "source": [
    "visits = pd.read_csv(\"../input/air_visit_data.csv\",\n",
    "                     parse_dates=[\"visit_date\"],\n",
    "                     index_col=[\"visit_date\"])\n",
    "visits = visits.sort_index()\n",
    "\n",
    "# Drop the missing stores\n",
    "for store in missing_stores:\n",
    "    visits = visits[ visits[\"air_store_id\"] != store ]\n",
    "\n",
    "# print some basic info\n",
    "print(\"Total visits:\",visits.shape[0])\n",
    "print(\"Number of unique stores after drop:\",visits[\"air_store_id\"].unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visits per day, week, and month\n",
    "\n",
    "Here we get customers:\n",
    " * each day\n",
    " * average for this day of the week\n",
    " * weekly average\n",
    " * monthly average\n",
    " * trend for last week, last month, and last 3 months (Taylor series)\n",
    " \n",
    "This is not the most pretty or efficient way of doing things, but at the moment calculating all of the above in one big loop using relative dates, working gradually into the past for 6 months. Note that here a month is defined as exactly 4 weeks. (The full range of data will be used later for the \"average from other stores\" section.)\n",
    "\n",
    "**Important note:** Both no customer and shop closed days are missing in the dataset (visits=0). We ignore those days when taking averages, since they are also ignored in the test-set scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning Customer Visits...\n",
      "Processing 0 months before\n",
      "Processing 1 months before\n",
      "Processing 2 months before\n",
      "Processing 3 months before\n",
      "Processing 4 months before\n",
      "Processing 5 months before\n"
     ]
    }
   ],
   "source": [
    "print(\"Scanning Customer Visits...\")\n",
    "\n",
    "# set a double index so we can look up date/store easily\n",
    "visits = visits.reset_index().set_index([\"visit_date\", \"air_store_id\"]).sort_index()\n",
    "\n",
    "# Make a new dataframe containing the visits data we will append to \"stores\"\n",
    "preds = pd.DataFrame(index = stores.index)\n",
    "\n",
    "# start on the day before today\n",
    "day = today - one_day\n",
    "\n",
    "# ignore division by zero errors\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "\n",
    "for months_before in range(6):\n",
    "    print(\"Processing\", months_before, \"months before\")\n",
    "    \n",
    "    for weeks_before in range(4):\n",
    "\n",
    "        for days_before in range(7):\n",
    "\n",
    "            visits_day = []\n",
    "            \n",
    "            # get visit data for EACH store, if today is before the prediction cutoff\n",
    "            for store in stores.index:\n",
    "                if ((day,store) in visits.index) and (day < prediction_date):\n",
    "                    visits_day.append(visits.loc[(day,store)][\"visitors\"])\n",
    "                else:\n",
    "                    visits_day.append(0)\n",
    "                    \n",
    "            # also keep track of \"store open / closed\" for averaging purposes\n",
    "            visits_day_count = [1 if i > 0 else 0 for i in visits_day]\n",
    "\n",
    "            # If in the first 7 days, store daily data\n",
    "            # (not currently using)\n",
    "            # unpack a dictionary with ** to provide variable names for columns\n",
    "            #if months_before == 0 and weeks_before == 0:\n",
    "            #    preds = preds.assign(**{\"visits_D-\"+str(days_before + 1) : visits_day})\n",
    "                \n",
    "            # if correct day of the week, add to average for this weekday\n",
    "            if days_before == 6: \n",
    "                if weeks_before == 0:\n",
    "                    weekday = visits_day\n",
    "                    weekday_count = visits_day_count\n",
    "                else:\n",
    "                    weekday = np.add(weekday, visits_day)\n",
    "                    weekday_count = np.add(weekday_count, visits_day_count)\n",
    "                    \n",
    "            # add daily visits to weekly total\n",
    "            if days_before == 0:\n",
    "                visits_week = visits_day\n",
    "                visits_week_count = visits_day_count\n",
    "            else:\n",
    "                visits_week = np.add(visits_week, visits_day)\n",
    "                visits_week_count = np.add(visits_week_count, visits_day_count)\n",
    "                \n",
    "            # increment day\n",
    "            day = day - one_day\n",
    "        \n",
    "        # Store weekly data (first month only)\n",
    "        if months_before == 0:\n",
    "            visits_week_ave = np.divide(visits_week, visits_week_count)\n",
    "            preds = preds.assign(**{\"visits_W-\"+str(weeks_before) : visits_week_ave})\n",
    "        \n",
    "        # add weekly visits to monthly total\n",
    "        if weeks_before == 0:\n",
    "            visits_month = visits_week\n",
    "            visits_month_count = visits_week_count\n",
    "        else:\n",
    "            visits_month = np.add(visits_month, visits_week)\n",
    "            visits_month_count = np.add(visits_month_count, visits_week_count)\n",
    "        \n",
    "    # Store monthly data\n",
    "    visits_month_ave = np.divide(visits_month, visits_month_count)\n",
    "    preds = preds.assign(**{\"visits_M-\"+str(months_before) : visits_month_ave})\n",
    "    \n",
    "    # Store average visits on this day of the week for each month\n",
    "    weekday_ave = np.divide(weekday, weekday_count)\n",
    "    preds = preds.assign(**{\"atw_M-\"+str(months_before) : weekday_ave})\n",
    "    \n",
    "    # Get overall average for this weekday\n",
    "    if months_before == 0:\n",
    "        weekday_total = weekday\n",
    "        weekday_total_count = weekday_count\n",
    "    else:\n",
    "        weekday_total = np.add(weekday_total, weekday)\n",
    "        weekday_total_count = np.add(weekday_total_count, weekday_count)\n",
    "        \n",
    "weekday_total_ave = np.divide(weekday_total, weekday_total_count)\n",
    "preds = preds.assign(atw_all = weekday_total_ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep quarterly variables\n",
    "first_three_months = preds[\"visits_M-0\"].add(preds[\"visits_M-1\"]).add(preds[\"visits_M-2\"])\n",
    "next_three_months = preds[\"visits_M-3\"].add(preds[\"visits_M-4\"]).add(preds[\"visits_M-5\"])\n",
    "\n",
    "# assign\n",
    "preds = preds.assign(trend_week = preds[\"visits_W-0\"].div(preds[\"visits_W-1\"]))\n",
    "preds = preds.assign(trend_month = preds[\"visits_M-0\"].div(preds[\"visits_M-1\"]))\n",
    "preds = preds.assign(trend_3_months = first_three_months.div(next_three_months))\n",
    "\n",
    "#preds\n",
    "\n",
    "# get rid of any inf values from all that division by zero\n",
    "# XGB can still handle nan values\n",
    "preds.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all of the above with stores data\n",
    "stores = pd.concat([stores, preds], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holidays\n",
    "\n",
    "** There are no holidays in the test set except golden week. Forget about this. **\n",
    "\n",
    "If prediction date is a holiday, assign the average fractional increase (or decrease) for each store and each genre during a holiday. Otherwise, fill with unity.\n",
    "\n",
    "Actually this is not needed for the test set, but may help our model avoid mistakes by learning wrong things from holiday days in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holidays = pd.read_csv(\"../input/date_info.csv\",\n",
    "#                       parse_dates = [\"calendar_date\"],\n",
    "#                       index_col = [\"calendar_date\"])\n",
    "\n",
    "# day_of_week = holidays.loc[prediction_date, \"day_of_week\"]\n",
    "# holiday = holidays.loc[prediction_date, \"holiday_flg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores.shape: (821, 25)\n"
     ]
    }
   ],
   "source": [
    "# # sum visitors for each store during non-holiday and during holiday\n",
    "# # to get the \"effect\" of a holiday for this store\n",
    "\n",
    "# # quick function for averages\n",
    "# def average(alist):\n",
    "#     return sum(alist) / max( len(alist), 1 )\n",
    "\n",
    "# if holiday:\n",
    "#     print(\"Calculating Holiday Info...\")\n",
    "    \n",
    "#     stores_average = []\n",
    "    \n",
    "#     print(\"Scanning over\", len(stores.index), \"stores\")\n",
    "#     count = 0    \n",
    "#     for store in stores.index:\n",
    "#         normal_visitors = []\n",
    "#         holiday_visitors = []\n",
    "        \n",
    "#         count += 1\n",
    "#         if count % 100 == 0:\n",
    "#             print (\"Scanned\", count, \"stores\")\n",
    "        \n",
    "#         # scan all days\n",
    "#         day = prediction_date\n",
    "#         while day > datetime(2016,1,1):\n",
    "#             day -= one_day\n",
    "            \n",
    "#             if (day,store) in visits.index:\n",
    "#                 if (holidays.loc[day, \"holiday_flg\"]):\n",
    "#                     holiday_visitors.append(visits.loc[(day,store)][0])\n",
    "#                 else:\n",
    "#                     normal_visitors.append(visits.loc[(day,store)][0])\n",
    "\n",
    "#         # calculate the average increase in visitors during a holiday\n",
    "#         if len(holiday_visitors) > 1 and len(normal_visitors) > 1:\n",
    "#             stores_average.append( average(holiday_visitors) / average(normal_visitors) )\n",
    "#         else:\n",
    "#             stores_average.append(1.)\n",
    "    \n",
    "#     stores = stores.assign(holiday_effect = stores_average)\n",
    "    \n",
    "# #else:\n",
    "# #    stores = stores.assign(holiday_effect = np.ones(len(stores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get average increase for genre\n",
    "# if holiday:\n",
    "    \n",
    "#     genre_name = []\n",
    "#     genre_effect = []\n",
    "\n",
    "#     for genre in stores[\"air_genre_name\"].unique():\n",
    "#         this_genre = stores[ stores[\"air_genre_name\"] == genre ]\n",
    "#         genre_name.append(genre)\n",
    "\n",
    "#         holiday_visitors = []\n",
    "#         normal_visitors = []\n",
    "        \n",
    "#         # scan all stores\n",
    "#         for store in this_genre.index:\n",
    "#             # scan all days\n",
    "#             day = prediction_date\n",
    "#             while day > datetime(2016,1,1):\n",
    "#                 day -= one_day\n",
    "                \n",
    "#                 if (day, store) in visits.index:\n",
    "#                     if (holidays.loc[day, \"holiday_flg\"]):\n",
    "#                         holiday_visitors.append(visits.loc[(day,store)][0])\n",
    "#                     else:\n",
    "#                         normal_visitors.append(visits.loc[(day,store)][0])\n",
    "\n",
    "#         # calculate the average increase in visitors during a holiday\n",
    "#         if len(holiday_visitors) > 1 and len(normal_visitors) > 1:\n",
    "#             genre_average = average(holiday_visitors) / average(normal_visitors)\n",
    "#         else:\n",
    "#             genre_average = 1.\n",
    "            \n",
    "#         print(genre, this_genre.shape[0], \"Holiday effect:\", genre_average)\n",
    "#         genre_effect.append(genre_average)\n",
    "        \n",
    "    \n",
    "#     # Assign to stores\n",
    "#     genre_effects = pd.DataFrame({\"genre\":genre_name, \"effect\":genre_effect}).set_index(\"genre\")\n",
    "#     stores = stores.assign(holiday_genre_effect = stores[\"air_genre_name\"].map(lambda x: genre_effects.loc[x,\"effect\"]))\n",
    "            \n",
    "# #else:\n",
    "# #    stores = stores.assign(holiday_genre_effect = np.ones(len(stores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average info from other stores\n",
    "\n",
    "Get the effective customer multiplier for various situations from all our data. This gives some hint for how to treat each genre and location.\n",
    "\n",
    "For average over:\n",
    "* Stores in this location (todofuken AND kushi)\n",
    "* Stores in this genre\n",
    "\n",
    "... we want the:\n",
    "* Multiplier for this month\n",
    "* Multiplier for this day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine low-population labels\n",
    "def get_low_pop(series):\n",
    "    to_drop = []\n",
    "    for name in series.index:\n",
    "        if series[name] < 5:\n",
    "            to_drop.append(name)\n",
    "    return to_drop\n",
    "        \n",
    "genres_to_drop = get_low_pop(stores[\"air_genre_name\"].value_counts())\n",
    "kushi_to_drop = get_low_pop(stores[\"kushi\"].value_counts())\n",
    "\n",
    "stores[\"air_genre_name\"] = stores[\"air_genre_name\"].map(lambda x: x if x not in genres_to_drop else \"genre_dropped\")\n",
    "stores[\"kushi\"] = stores[\"kushi\"].map(lambda x: x if x not in kushi_to_drop else \"kushi_dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average stuff over all stores\n",
    "\n",
    "def normalize(li):\n",
    "    \"\"\"\n",
    "    Simple function to normalize a list.\n",
    "    \"\"\"\n",
    "    average = np.sum(li) / len(li)\n",
    "    li = np.divide(li, average)\n",
    "    return li\n",
    "\n",
    "\n",
    "def calc_effects(col):\n",
    "    \"\"\"\n",
    "    Takes in a column property of \"stores\", e.g. area or genre,\n",
    "    and calculates the normalized effect of each month and weekday.\n",
    "    \"\"\"\n",
    "    \n",
    "    property_name = col.name\n",
    "    \n",
    "    # prepare dataframe to hold the effects of this property\n",
    "    effects = pd.DataFrame(index = col.unique(), columns = [\"months\", \"weekdays\"])\n",
    "    \n",
    "    # For each label of property\n",
    "    for label in effects.index:\n",
    "        \n",
    "        print(\"Calculating for\", label)\n",
    "        \n",
    "        # get sublist of stores with this label\n",
    "        store_list = stores[ stores[property_name] == label ].index\n",
    "        \n",
    "        # prepare temp variables for speed (DF access is slow)\n",
    "        months = np.zeros(12)\n",
    "        months_count = np.zeros(12)\n",
    "        weekdays = np.zeros(7)\n",
    "        weekdays_count = np.zeros(7)\n",
    "\n",
    "        # Loop over all dates\n",
    "        dates = pd.date_range(datetime(2016,1,1), prediction_date - one_day)\n",
    "        for date in dates:\n",
    "\n",
    "            # Count number of visitors, and number of stores open\n",
    "            total_visitors = 0\n",
    "            total_open = 0\n",
    "            for store in store_list:\n",
    "                if (date,store) in visits.index:\n",
    "                    total_open += 1\n",
    "                    total_visitors += visits.loc[(date,store)][\"visitors\"]\n",
    "\n",
    "            # fill for averages\n",
    "            months[date.month - 1] += total_visitors\n",
    "            months_count[date.month - 1] += total_open\n",
    "            weekdays[date.weekday()] += total_visitors\n",
    "            weekdays_count[date.weekday()] += total_open\n",
    "\n",
    "        # Calculate normalized effects\n",
    "        months_ave = normalize( np.divide(months, months_count) )\n",
    "        weekdays_ave = normalize( np.divide(weekdays, weekdays_count) )\n",
    "        \n",
    "        # Store info to DF\n",
    "        effects.loc[label,\"months\"] = months_ave\n",
    "        effects.loc[label, \"weekdays\"] = weekdays_ave\n",
    "        \n",
    "    # return effects DF\n",
    "    return effects\n",
    "\n",
    "\n",
    "def append_to_stores(effects, col):\n",
    "    stores[str(col.name + \"_monthly_effects\")] = col.map(lambda x: effects.loc[x, \"months\"][today.month-1])\n",
    "    stores[str(col.name + \"_weekday_effects\")] = col.map(lambda x: effects.loc[x, \"weekdays\"][today.weekday()])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run calculation & save\n",
    "if (run_calculation):\n",
    "    kushi_effects = calc_effects(stores[\"kushi\"])\n",
    "    todofuken_effects = calc_effects(stores[\"todofuken\"])\n",
    "    genre_effects = calc_effects(stores[\"air_genre_name\"])\n",
    "    kushi_effects.to_csv(\"histograms/kushi_effects.csv\")\n",
    "    todofuken_effects.to_csv(\"histograms/todofuken_effects.csv\")\n",
    "    genre_effects.to_csv(\"histograms/genre_effects.csv\")\n",
    "\n",
    "# Else read from old calculation\n",
    "else:\n",
    "    kushi_effects = pd.read_csv(\"histograms/kushi_effects.csv\", index_col=0)\n",
    "    todofuken_effects = pd.read_csv(\"histograms/todofuken_effects.csv\", index_col=0)\n",
    "    genre_effects = pd.read_csv(\"histograms/genre_effects.csv\", index_col=0)\n",
    "    \n",
    "    # Unfortunately it seems pandas can't store / read lists inside of cells properly\n",
    "    # ugly hack needed...\n",
    "    kushi_effects[\"months\"] = kushi_effects[\"months\"].map( lambda x: np.fromstring(x[2:-1], sep=\" \") )\n",
    "    todofuken_effects[\"months\"] = todofuken_effects[\"months\"].map( lambda x: np.fromstring(x[2:-1], sep=\" \") )\n",
    "    genre_effects[\"months\"] = genre_effects[\"months\"].map( lambda x: np.fromstring(x[2:-1], sep=\" \") )\n",
    "    kushi_effects[\"weekdays\"] = kushi_effects[\"weekdays\"].map( lambda x: np.fromstring(x[2:-1], sep=\" \") )\n",
    "    todofuken_effects[\"weekdays\"] = todofuken_effects[\"weekdays\"].map( lambda x: np.fromstring(x[2:-1], sep=\" \") )\n",
    "    genre_effects[\"weekdays\"] = genre_effects[\"weekdays\"].map( lambda x: np.fromstring(x[2:-1], sep=\" \") )\n",
    "\n",
    "append_to_stores(kushi_effects, stores[\"kushi\"])\n",
    "append_to_stores(todofuken_effects, stores[\"todofuken\"])\n",
    "append_to_stores(genre_effects, stores[\"air_genre_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign min, med, max for related stores (in this area & genre)\n",
    "gb = stores.reset_index().groupby([\"air_genre_name\",\"kushi\"])\n",
    "recent_visits = visits.reset_index().copy()\n",
    "recent_visits = recent_visits[ recent_visits[\"visit_date\"] > prediction_date - timedelta(weeks=15) ]\n",
    "recent_visits = recent_visits.set_index([\"air_store_id\"])\n",
    "stores[\"genarea_min\"] = 0\n",
    "stores[\"genarea_med\"] = 0\n",
    "stores[\"genarea_max\"] = 0\n",
    "\n",
    "# loop over all genre-area pairs\n",
    "for name, group in gb:\n",
    "    related_stores = group[\"air_store_id\"].values\n",
    "\n",
    "    visitors = []\n",
    "    for store in related_stores:\n",
    "        visitors.extend( recent_visits.loc[store,\"visitors\"].values )\n",
    "\n",
    "    min_ = np.percentile(visitors, 16)\n",
    "    med_ = np.percentile(visitors, 50)\n",
    "    max_ = np.percentile(visitors, 84)\n",
    "    \n",
    "    for store in related_stores:\n",
    "        stores.loc[store,\"genarea_min\"] = min_\n",
    "        stores.loc[store,\"genarea_med\"] = med_\n",
    "        stores.loc[store,\"genarea_max\"] = max_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden Week\n",
    "* Calculate average effect of golden week on that store's\n",
    "  * genre\n",
    "  * area (todofuken, kushi.\n",
    "  \n",
    "This is important as it will be in the test set. We should probably try and calculate the average effect of golden week for each day inside of golden week. Note also that this might conflict with general holidays information. **For now, placeholder function is in place.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GW is 4/29 -> 5/5\n",
    "# in 2016 this is FRI ~ THU (MON is a \"work day\")\n",
    "# in 2017 this is SAT ~ FRI (MON, TUE are \"work days\")\n",
    "\n",
    "def check_golden_week(the_date):\n",
    "    if (the_date >= datetime(2016,4,29)) and (the_date < datetime(2016,5,6)):\n",
    "        return 1\n",
    "    elif (the_date >= datetime(2017,4,29)) and (the_date < datetime(2017,5,6)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "if check_golden_week(today):\n",
    "    stores[\"gw_effect\"] = stores[\"todofuken\"].map(lambda x: 0.8 if x == \"Tōkyō-to\" else 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hotting\n",
    "Note that low-populated labels have already been dropped earlier to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictors with shape (821, 83) saved to CSV\n"
     ]
    }
   ],
   "source": [
    "# Split numeric and text data\n",
    "X_numeric = stores.select_dtypes(exclude=['object']).copy()\n",
    "X_text = stores.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# get one-hot columns from text data\n",
    "X_onehot = pd.get_dummies(X_text)\n",
    "\n",
    "# recombine\n",
    "X_final = pd.concat([X_numeric, X_onehot], axis=1)\n",
    "\n",
    "# save\n",
    "X_final.to_csv(\"tabular_data/\" + today.strftime(\"%Y_%m_%d\") + \".csv\")\n",
    "print(\"Final predictors with shape\", X_final.shape, \"saved to CSV\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
